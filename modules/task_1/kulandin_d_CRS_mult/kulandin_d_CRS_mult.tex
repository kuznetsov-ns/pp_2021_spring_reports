\documentclass{report}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{luainputenc}
\usepackage[english, russian]{babel}
\usepackage[pdftex]{hyperref}
\usepackage[14pt]{extsizes}
\usepackage{listings}
\usepackage{color}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{pgfplots}

\geometry{a4paper,top=2cm,bottom=3cm,left=2cm,right=1.5cm}
\setlength{\parskip}{0.5cm}

\lstset{
    language=C++,
	numbers=left,
	basicstyle=\footnotesize,
	keywordstyle=\color{blue}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	commentstyle=\color{green}\ttfamily,
	morecomment=[l][\color{magenta}]{\#}, 
	tabsize=2,
	title=\lstname,       
}

\makeatletter
\renewcommand\@biblabel[1]{#1.\hfil}
\makeatother

\begin{document}

\begin{titlepage}

\begin{center}
Министерство науки и высшего образования Российской Федерации
\end{center}

\begin{center}
Федеральное государственное автономное образовательное учреждение высшего образования \\
Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского
\end{center}

\begin{center}
Институт информационных технологий, математики и механики
\end{center}

\vspace{4em}

\begin{center}
\textbf{\LargeОтчет по лабораторной работе} \\
\end{center}
\begin{center}
\textbf{\LargeУмножение разреженных матриц. Элементы комплексного типа. Формат хранения матрицы – строковый (CRS)} \\
\end{center}

\vspace{4em}

\newbox{\lbox}
\savebox{\lbox}{\hbox{text}}
\newlength{\maxl}
\setlength{\maxl}{\wd\lbox}
\hfill\parbox{7cm}{
\hspace*{5cm}\hspace*{-5cm}\textbf{Выполнил:} \\ студент группы 381806-2 \\ Куландин Д.С.\\
\\
\hspace*{5cm}\hspace*{-5cm}\textbf{Проверил:}\\ доцент кафедры МОСТ, \\ кандидат технических наук \\ Сысоев А. В.
}

\vspace{\fill}

\begin{center} Нижний Новгород \\ 2021 \end{center}

\end{titlepage}

\setcounter{page}{2}

% Содержание
\tableofcontents
\newpage

% Введение
\section*{Введение}
\addcontentsline{toc}{section}{Введение}
Матрицы, которые содержат в основном нулевые значения, называются разреженными, в отличие от матриц, где большинство значений отличны от нуля, которые называются плотными.

Большие разреженные матрицы распространены в общем и особенно в прикладном машинном обучении, например, в данных, которые содержат счетчики, кодировки данных, которые отображают категории в счетчики, и даже во всех подполях машинного обучения, таких как обработка естественного языка.

В вычислительном отношении дорого представлять и работать с разреженными матрицами, как если бы они были плотными, и можно добиться значительного улучшения производительности, используя представления и операции, которые специально обрабатывают разреженность матриц.

\newpage

% Постановка задачи
\section*{Постановка задачи}
\addcontentsline{toc}{section}{Постановка задачи}
В рамках лабораторной работы нужно реализовать алгоритм умножения разреженных матриц с элементами комплексного типа, используя строковый формат хранения.

\par Для этого нужно выполнить следующую последовательность действий:

\begin{enumerate}
\item Изучить способы хранения разреженных матриц.
\item Разработать программную инфраструктуру для проведения экспериментов - генератора разреженных матриц с заданным процентом ненулевых элементов, метод для проверки корректности умножения матриц.
\item Разработать последовательную версию алгоритма.
\item Реализовать распараллеливание с использованием OpenMP, TBB и std::thread.
\item Осуществить проверку корректности работы алгоритмов с помощью тестов, созданных с использованием Google C++ Testing Framework.
\item Провести вычислительные эксперименты и сделать соответствующие выводы.
\end{enumerate}
\newpage

% Метод решения
\section*{Метод решения}
\addcontentsline{toc}{section}{Метод решения}
Хранение разреженной матрицы осуществляется с помощью структуры, которая содержит следующие поля:
\begin{enumerate}
\item Размерность матрицы.
\item Массив для хранения ненулевых элементов.
\item Массив, хранящий номера столбцов для каждого элемента.
\item Массив, содержащий диапазон индексов в остальных массивах, для определения какой строке принадлежит соответствующий элемент.
\end{enumerate}
В качестве тестовых матриц будут использоваться случайно созданные матрицы с заданным процентом заполнения. Проверка корректности алгоритма умножения проводится с помощью метода, который переводит разреженную матрицу в плотную и выполняет стандартное умножение плотных матриц за $\mathcal{O}(n^3)$

\par Метод реализующий алгоритм умножения принимает на вход две разреженных матрицы A и B в строковом формате хранения и возвращает их произведение - разреженную матрицу С. Опишем его шаги:
\begin{enumerate}
\item Создаем три вспомогательных массива для хранения результирующей матрицы С.
\item Транспонируем матрицу B алгоритмом Густавсона.
\item В цикле идем по строками матрицы A, во внутреннем цикле идем по всем строками транспонированной матрицы B.
\item Вычисляем скалярное произведение двух зафиксированных разреженных строк матриц A и транспонированной B. Если оно отлично от 0, тогда добавляем его и номер столбца в соответствующие массивы.
\item После прохода по каждой строке из транспонированной B записываем в массив индексов текущее количество ненулевых элементов в результирующей матрицы.
\end{enumerate}
По окончании работы алгоритма, в вспомогательных массивах будет храниться результирующая разреженная матрица С.

\newpage

\section*{Скалярное умножение разреженных строк}
\addcontentsline{toc}{section}{Скалярное умножение разреженных строк}

Наивная реализация заключается в проходе по массиву столбцов для матрицы А и проходу по массиву столбцов транспонированной матрицы В. Если номера столбцов совпадут, то добавляем в результат произведение двух элементов. Данное решение будет работать за $\mathcal{O}(n^2)$.

\par Можно заметить, что для фиксированной строки номера столбцов будут отсортированны по возрастанию. Это значит, что подсчет скалярного произведения возможно реализовать с помощью двух указателей, как это сделано в функции слияния в сортировке слиянием.
То есть у нас будет два указателя, первый указывает на начало массива столбцов фиксированной строки матрицы А, второй - транспонированной матрицы В. В цикле идем первым указателем по массиву, второй указатель будем сдвигать вправо пока значение меньше, чем значение первого указателя. Если значение первого и второго указанных элементов совпали, то добавляем произведение в ответ. Данное решение будет работать за $\mathcal{O}(n)$.
\par При реализации параллельной функции опытным путём было замечено, что данная реализация является хотспотом и нуждается в доработке.
\par Для компенсации времени работы программы было принято решение использовать дополнительную память. Таким образом, мы компенсируем время работы программы, используя память. Введем новый массив, который будет хранить в себе информацию о посещенных нами столбцах при проходе по массиву столбцов матрицы А. При проходе по массиву столбцов транспонированной матрицы В достаточно лишь посмотреть на значение в вспомогательном массиве. Данное решение также будет работать за $\mathcal{O}(n)$, затраты по памяти будут $\mathcal{O}(n)$. Экспериментально доказано, что данное решение работает быстрее, чем предыдущее.
\par Внутри реализованной секции будет присутствовать условие на проверку равенства элемента вспомогательного массива -1. При запуске Intel® VTune™ Profiler данное условие будет являться хотспотом. Устранить данную проблему можно, используя дополнительный элемент в вспомогательном массиве, который будет равен 0. Тогда изначально вспомогательный массив будет заполнен не -1, а индексом в массиве, где содержится 0. Тогда в случае несовпадения столбцов будет производиться умножение на 0, то есть результат не изменится. Данное предположение позволило получить небольшой прирост в производительности и избавиться от хотспотов в реализации.
\newpage
% Схема распараллеливания
\section*{Схема распараллеливания}
\addcontentsline{toc}{section}{Схема распараллеливания}
Рассмотрим параллельную реализацию алгоритма умножения разреженных матриц.

\par Идея распараллеливания алгоритма достаточно очевидна. Во внешнем цикле перебираем строки матрицы А и далее работаем с ними независимо.
\par Внутренний цикл бесполезно распараллеливать, потому что это повлечет за собой дополнительные накладные расходы и ускорение будет гораздо меньше ожидаемого.
\par При распараллеливании внешнего цикла критической секцией будет являться запись в конец массивов индексов и столбцов. Для реализации доступа к критической секции придется использоваться мьютексы, это может сильно повлиять на время выполнения программы.
\par Оптимизация будет заключаться в создании вспомогательных массивов для каждой строки результирующего массива. Таким образом, запись уже не будет являться критической секцией, так как к каждой строке доступ имеет только один поток.
После выполнения параллельной секции, необходимо слить данные из созданных массивов в один.
Вновь, с помощью использования дополнительной памяти мы компенсировали время работы программы.

\newpage

% Описание программной реализации
\section*{Описание программной реализации}
\addcontentsline{toc}{section}{Описание программной реализации}
Разреженная матрица хранится в виде следующего класса:

\begin{lstlisting}
class SparseMatrix {
 public:
    explicit SparseMatrix(int _size = 0);
    SparseMatrix(const SparseMatrix & a);
    SparseMatrix(const std::vector<std::complex<double>> & a, int _size);
    ~SparseMatrix() = default;
    SparseMatrix& operator=(const SparseMatrix & a);
    SparseMatrix operator*(const SparseMatrix & a);
    std::vector<std::complex<double>> getValues() const;
    std::vector<int> getCols() const;
    std::vector<int> getPointers() const;
    int getSize() const;
    void setSize(const int size);
    void setValues(const std::vector<std::complex<double>> & val);
    void setCols(const std::vector<int> & col);
    void setPointers(const std::vector<int> & pointers);
    std::vector<std::complex<double>> getDenseMatrix() const;
    SparseMatrix transposition() const;
    SparseMatrix openMPMultiplication(const SparseMatrix & a);
    SparseMatrix TBBMultiplication(const SparseMatrix & a, const int threads);
    SparseMatrix threadMultiplication(const SparseMatrix & a);
 private:
    int size;
    std::vector<std::complex<double>> values;
    std::vector<int> cols, pointers;
};
\end{lstlisting}
\par Реализация параллельного алгоритма умножения разреженных матриц представлена в функциях:
\begin{lstlisting}
SparseMatrix openMPMultiplication(const SparseMatrix & a);
SparseMatrix TBBMultiplication(const SparseMatrix & a, const int threads);
SparseMatrix threadMultiplication(const SparseMatrix & a);
\end{lstlisting}
\par В качестве входных параметров передаются матрицы, которые нужно перемножить, а выходным является результирующая матрица.
\newpage

% Подтверждение корректности
\section*{Подтверждение корректности}
\addcontentsline{toc}{section}{Подтверждение корректности}
Для проверки эффективности и корректности работы программы используются тесты, написанные с помощью Google Testing Framework.
\par Они проверяют эффективность работы параллельной схемы (путем сравнения времени выполнения параллельной и последовательной реализации) и корректность вычислений (проверкой правильности формирования массивов разреженной матрицы и проверкой точности - операцией скалярного умножения векторов).

\par Успешное прохождение всех тестов подтверждает корректность работы написанной программы.

\newpage

% Результаты экспериментов
\section*{Результаты экспериментов}
\addcontentsline{toc}{section}{Результаты экспериментов}
Вычислительные эксперименты для оценки эффективности параллельного алгоритма умножения разреженных матриц проводились на оборудовании со следующей аппаратной конфигурацией:

\begin{itemize}
\item Процессор: Intel Core i5-7500, 3.7 GHz, 4 ядра, HT: Off
\item Оперативная память: 24 Гб
\item ОС: Майкрософт Windows 10
\end{itemize}

\begin{tikzpicture}
\begin{axis}[
    title = Результаты экспериментов при размере матрицы $4000$ * $4000$,
	xlabel = {Количество ненулевых элементов $(10^3)$},
	ylabel = {Время, сек},
	width = 400, 
	height = 400,
	legend pos = north west, 
    ymin = 0.6, 
    xmin = 0,
    grid = major,
]
\legend{ 
	Последовательная реализация,
	openMP реализация,
	TBB реализация,
	std::thread реализация,
};
\addplot coordinates {
	(4, 2.538)
	(8, 2.659)
	(12, 2.653)
	(16, 2.753)
	(20, 2.760)
	(24, 2.763)
	(28, 2.715)
	(32, 2.759)
	(36, 2.745)
	(40, 2.831)
	(80, 3.113)
	(120, 3.534)
	(160, 4.016)
	(200, 4.314)
	(240, 4.808)
	(280, 5.338)
	(320, 6.123)
	(360, 6.603)
	(400, 7.285)
};
\addplot coordinates {
	(4, 0.685)
	(8, 0.695)
	(12, 0.722)
	(16, 0.759)
	(20, 0.734)
	(24, 0.772)
	(28, 0.76)
	(32, 0.768)
	(36, 0.764)
	(40, 0.822)
	(80, 0.898)
	(120, 1.066)
	(160, 1.2)
	(200, 1.4)
	(240, 1.538)
	(280, 1.596)
	(320, 1.73)
	(360, 1.817)
	(400, 2.10)
};
\addplot coordinates {
	(4, 0.693)
	(8, 0.69)
	(12, 0.707)
	(16, 0.741)
	(20, 0.754)
	(24, 0.763)
	(28, 0.744)
	(32, 0.754)
	(36, 0.774)
	(40, 0.799)
	(80, 0.901)
	(120, 1.078)
	(160, 1.195)
	(200, 1.375)
	(240, 1.466)
	(280, 1.6)
	(320, 1.757)
	(360, 1.853)
	(400, 2.086)
};
\addplot coordinates {
	(4, 0.697)
	(8, 0.707)
	(12, 0.716)
	(16, 0.713)
	(20, 0.725)
	(24, 0.818)
	(28, 0.770)
	(32, 0.845)
	(36, 0.806)
	(40, 0.790)
	(80, 1.010)
	(120, 1.076)
	(160, 1.208)
	(200, 1.449)
	(240, 1.631)
	(280, 1.651)
	(320, 1.779)
	(360, 1.973)
	(400, 2.365)
};
\end{axis}
\end{tikzpicture}

\par По данным экспериментов можно сделать вывод, что параллельный алгоритм работает намного быстрее, чем последовательный. Максимальное ускорение - 3.5 раза, теоретически максимальное ускорение - 4 раза. Как можно видеть по результатам экспериментов, среди трех использованных стандартов распараллеливания нет единственно быстрого, при разных размерностях поведение различно.
\newpage

% Заключение
\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}
В ходе выполнения лабораторной работы был реализован последовательный и параллельный вариант алгоритма умножения разреженных матриц, хранящихся в формате CRS. Было применено некоторое количество как алгоритмических, так и аппаратных оптимизаций, которые помогли достигнуть поставленных перформанс целей. При тестировании программы в ННГУ на компьюетере с 12 потоками производительности составляла 10.5 раз, что по моему скромному мнению является хорошим результатом.

\newpage

% Список литературы
\begin{thebibliography}{1}
\addcontentsline{toc}{section}{Список литературы}
\bibitem{Sysoev} Сысоев А.В., Мееров И.Б., Свистунов А.Н., Курылев А.Л., Сенин А.В., Шишков А.В., Корняков К.В., Сиднев А.А. «Параллельное программирование в системах с общей памятью. Инструментальная поддержка». Учебно-методические материалы по программе повышения квалификации «Технологии высокопроизводительных вычислений для обеспечения учебного процесса и научных исследований». Нижний Новгород, 2007, 110 с. 
\bibitem{Gergel}
Гергель В. П. Теория и практика параллельных вычислений. – 2007. 
\bibitem{Gergel}
Гергель В. П., Стронгин Р. Г. Основы параллельных вычислений для многопроцессорных вычислительных систем. – 2003.
\bibitem{cppreference} cppreference [Электронный ресурс] // https://en.cppreference.com/w/
\end{thebibliography}

\newpage

\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}

\par Последовательная реализация представлена в виде перегрузки оператора умножения, реализации с помощью OpenMP, TBB и std::thread представлены с помощью функций 

\begin{lstlisting}
// sparsematrix.h
// Copyright 2021 Kulandin Denis
#ifndef MODULES_TASK_KULANDIN_D_MATRIX_CRS_COMPLEX_SPARSEMATRIX_H_
#define MODULES_TASK_KULANDIN_D_MATRIX_CRS_COMPLEX_SPARSEMATRIX_H_

#include <vector>
#include <complex>
#include "tbb/blocked_range.h"
#include "tbb/parallel_for.h"
#include "tbb/task_scheduler_init.h"
#include "tbb/tick_count.h"

class SparseMatrix {
 public:
    explicit SparseMatrix(int _size = 0);
    SparseMatrix(const SparseMatrix & a);
    SparseMatrix(const std::vector<std::complex<double>> & a, int _size);
    ~SparseMatrix() = default;
    SparseMatrix& operator=(const SparseMatrix & a);
    SparseMatrix operator*(const SparseMatrix & a);
    std::vector<std::complex<double>> getValues() const;
    std::vector<int> getCols() const;
    std::vector<int> getPointers() const;
    int getSize() const;
    void setSize(const int size);
    void setValues(const std::vector<std::complex<double>> & val);
    void setCols(const std::vector<int> & col);
    void setPointers(const std::vector<int> & pointers);
    std::vector<std::complex<double>> getDenseMatrix() const;
    SparseMatrix transposition() const;
    SparseMatrix openMPMultiplication(const SparseMatrix & a);
    SparseMatrix TBBMultiplication(const SparseMatrix & a, const int threads);
    SparseMatrix threadMultiplication(const SparseMatrix & a);
 private:
    int size;
    std::vector<std::complex<double>> values;
    std::vector<int> cols, pointers;
};

SparseMatrix generateRandomSparseMatrix(const int size,
                                        const int nonZeroElementsInEveryRow);
std::vector<std::complex<double>>
   stupidDenseMultiplication(const std::vector<std::complex<double>> & a,
                             const std::vector<std::complex<double>> & b,
                             const int size);

#endif  // MODULES_TASK_KULANDIN_D_MATRIX_CRS_COMPLEX_SPARSEMATRIX_H_
\end{lstlisting}
\begin{lstlisting}
// sparsematrix.cpp
// Copyright 2021 Kulandin Denis
#include <limits>
#include <random>
#include "../../../3rdparty/unapproved/unapproved.h"
#include "../../../modules/task_3/kulandin_d_matrix_CRS_complex/sparsematrix.h"

bool equalZero(const std::complex<double> & a) {
    return std::abs(a) < std::numeric_limits<double>::epsilon();
}

SparseMatrix::SparseMatrix(int _size) {
    size = _size;
}

SparseMatrix::SparseMatrix(const SparseMatrix & a) : size(a.size),
                                                     values(a.values),
                                                     cols(a.cols),
                                                     pointers(a.pointers) {}

SparseMatrix& SparseMatrix::operator=(const SparseMatrix & a) {
    if (this == &a) return *this;
    size = a.size;
    values = a.values;
    cols = a.cols;
    pointers = a.pointers;
    return *this;
}

SparseMatrix::SparseMatrix(const std::vector<std::complex<double>> & a, int n) {
    size = n;
    pointers.push_back(0);
    for (int i = 0; i < n; ++i) {
        for (int j = 0; j < n; ++j) {
            if (equalZero(a[i * n + j])) continue;
            values.push_back(a[i * n + j]);
            cols.push_back(j);
        }
        pointers.push_back(static_cast<int>(cols.size()));
    }
}

std::vector<std::complex<double>> SparseMatrix::getValues() const {
    return values;
}

std::vector<int> SparseMatrix::getCols() const {
    return cols;
}

std::vector<int> SparseMatrix::getPointers() const {
    return pointers;
}

int SparseMatrix::getSize() const {
    return size;
}

void SparseMatrix::setSize(const int size) {
    this->size = size;
}

void SparseMatrix::setValues(const std::vector<std::complex<double>> & val) {
    values = val;
}

void SparseMatrix::setCols(const std::vector<int> & col) {
    cols = col;
}

void SparseMatrix::setPointers(const std::vector<int> & pointers) {
    this->pointers = pointers;
}

std::vector<std::complex<double>> SparseMatrix::getDenseMatrix() const {
    std::vector<std::complex<double>> ans(
        size * size,
        std::complex<double>(0, 0));
    for (int row = 0; row < size; ++row) {
        for (int j = pointers[row]; j < pointers[row + 1]; ++j) {
            ans[row * size + cols[j]] = values[j];
        }
    }
    return ans;
}

SparseMatrix SparseMatrix::transposition() const {
    SparseMatrix a(size);
    std::vector<std::vector<int>> helpCols(size);
    std::vector<std::vector<std::complex<double>>> helpValues(size);
    for (int row = 0; row < size; ++row) {
        for (int j = pointers[row]; j < pointers[row + 1]; ++j) {
            helpCols[cols[j]].push_back(row);
            helpValues[cols[j]].push_back(values[j]);
        }
    }
    a.pointers.push_back(0);
    for (int i = 0; i < size; ++i) {
        for (int j = 0; j < static_cast<int>(helpCols[i].size()); ++j) {
            a.values.push_back(helpValues[i][j]);
            a.cols.push_back(helpCols[i][j]);
        }
        a.pointers.push_back(
            a.pointers.back() + static_cast<int>(helpCols[i].size()));
    }
    return a;
}

SparseMatrix SparseMatrix::operator*(const SparseMatrix & a) {
    if (size != a.getSize()) {
        throw(std::string)"Wrong matrix sizes";
    }
    SparseMatrix ans(size);
    ans.pointers.push_back(0);
    SparseMatrix a_t = a.transposition();
    auto a_t_values = a_t.getValues();
    auto a_t_cols = a_t.getCols();
    auto a_t_pointers = a_t.getPointers();
    std::vector<int> used(size, -1);
    std::complex<double> cur(0, 0);
    for (int row1 = 0; row1 < size; ++row1) {
        for (int row2 = 0; row2 < size; ++row2) {
            used.assign(size, -1);
            cur = {0, 0};
            for (int i = pointers[row1]; i < pointers[row1 + 1]; ++i) {
                used[cols[i]] = i;
            }
            for (int i = a_t_pointers[row2]; i < a_t_pointers[row2 + 1]; ++i) {
                if (used[a_t_cols[i]] == -1) continue;
                cur += values[used[a_t_cols[i]]] * a_t_values[i];
            }
            if (!equalZero(cur)) {
                ans.cols.emplace_back(row2);
                ans.values.emplace_back(cur);
            }
        }
        ans.pointers.emplace_back(static_cast<int>(ans.values.size()));
    }
    return ans;
}

SparseMatrix SparseMatrix::openMPMultiplication(const SparseMatrix & a) {
    if (size != a.getSize()) {
        throw(std::string)"Wrong matrix sizes";
    }
    SparseMatrix a_t  = a.transposition();
    auto a_t_values   = a_t.getValues();
    auto a_t_cols     = a_t.getCols();
    auto a_t_pointers = a_t.getPointers();
    std::vector<std::vector<int>> parallel_columns(size);
    std::vector<std::vector<std::complex<double>>> parallel_values(size);
    values.emplace_back(0, 0);
    int tmp = static_cast<int>(values.size() - 1);
    std::vector<int> used(size, tmp);
    std::complex<double> cur(0, 0);
    #pragma omp parallel
    {
        #pragma omp for private(used, cur) schedule(static)
        for (int row1 = 0; row1 < size; ++row1) {
            for (int row2 = 0; row2 < size; ++row2) {
                used.assign(size, tmp);
                cur = {0, 0};
                int start = pointers[row1];
                int finish = pointers[row1 + 1];
                for (int i = start; i < finish; ++i) {
                    used[cols[i]] = i;
                }
                start = a_t_pointers[row2];
                finish = a_t_pointers[row2 + 1];
                for (int i = start; i < finish; ++i) {
                    cur += values[used[a_t_cols[i]]] * a_t_values[i];
                }
                if (!equalZero(cur)) {
                    parallel_columns[row1].emplace_back(row2);
                    parallel_values[row1].emplace_back(cur);
                }
            }
        }
    }
    values.pop_back();
    int sum = 0;
    for (int i = 0; i < size; ++i) {
        sum += static_cast<int>(parallel_columns[i].size());
    }
    SparseMatrix ans(size);
    ans.cols.reserve(sum);
    ans.values.reserve(sum);
    ans.pointers.reserve(size + 1);
    ans.pointers.emplace_back(0);
    for (int i = 0; i < size; ++i) {
        std::copy(parallel_columns[i].begin(),
                  parallel_columns[i].end(),
                  std::back_inserter(ans.cols));
        std::copy(parallel_values[i].begin(),
                  parallel_values[i].end(),
                  std::back_inserter(ans.values));
        ans.pointers.emplace_back(static_cast<int>(ans.cols.size()));
    }
    return ans;
}

SparseMatrix SparseMatrix::TBBMultiplication(const SparseMatrix & a,
                                             const int threads) {
    if (size != a.getSize()) {
        throw(std::string)"Wrong matrix sizes";
    }
    SparseMatrix a_t = a.transposition();
    auto b_cols     = a_t.getCols();
    auto b_pointers = a_t.getPointers();
    auto b_val      = a_t.getValues();
    values.emplace_back(0, 0);
    int tmp = static_cast<int>(values.size() - 1);
    std::vector<std::vector<int>> parallel_columns(size);
    std::vector<std::vector<std::complex<double>>> parallel_values(size);
    tbb::task_scheduler_init init(threads);
    tbb::parallel_for(tbb::blocked_range<int>(0, size, 100),
        [&](const tbb::blocked_range<int> &r){
        std::vector<int> used(size, tmp);
        for (int row1 = r.begin(); row1 < r.end(); ++row1) {
            for (int row2 = 0; row2 < size; ++row2) {
                std::complex<double> cur(0, 0);
                int start = pointers[row1];
                int finish = pointers[row1 + 1];
                for (int i = start; i < finish; ++i) {
                    used[cols[i]] = i;
                }
                start = b_pointers[row2];
                finish = b_pointers[row2 + 1];
                for (int i = start; i < finish; ++i) {
                    cur += values[used[b_cols[i]]] * b_val[i];
                }
                if (!equalZero(cur)) {
                    parallel_columns[row1].emplace_back(row2);
                    parallel_values[row1].emplace_back(cur);
                }
               used.assign(size, tmp);
            }
        }
    });
    init.terminate();
    values.pop_back();
    int sum = 0;
    for (int i = 0; i < size; ++i) {
        sum += static_cast<int>(parallel_columns[i].size());
    }
    SparseMatrix ans(size);
    ans.cols.reserve(sum);
    ans.values.reserve(sum);
    ans.pointers.reserve(size + 1);
    ans.pointers.emplace_back(0);
    for (int i = 0; i < size; ++i) {
        std::copy(parallel_columns[i].begin(),
                  parallel_columns[i].end(),
                  std::back_inserter(ans.cols));
        std::copy(parallel_values[i].begin(),
                  parallel_values[i].end(),
                  std::back_inserter(ans.values));
        ans.pointers.emplace_back(static_cast<int>(ans.cols.size()));
    }
    return ans;
}

SparseMatrix generateRandomSparseMatrix(const int size,
                                        const int nonZeroElementsInEveryRow) {
    if (nonZeroElementsInEveryRow > size) {
        throw(std::string)"Wrong input arguments";
    }
    std::mt19937 gen;
    gen.seed(static_cast<unsigned int>(time(0)));
    SparseMatrix ans;
    ans.setSize(size);
    std::vector<std::complex<double>> val;
    std::vector<int> cols, pointers;
    pointers.push_back(0);
    for (int row = 0; row < size; ++row) {
        std::vector<bool> generatedCols(size, 0);
        int sum = 0;
        while (sum < nonZeroElementsInEveryRow) {
            int generated = gen() % size;
            if (!generatedCols[generated]) {
                generatedCols[generated] = 1;
                ++sum;
            }
        }
        for (int i = 0; i < size; ++i) {
            if (!generatedCols[i]) continue;
            cols.push_back(i);
            val.push_back(std::complex<double>(
                static_cast<double>(gen()) / gen(),
                static_cast<double>(gen()) / gen()));
        }
        pointers.push_back(static_cast<int>(cols.size()));
    }
    ans.setPointers(pointers);
    ans.setCols(cols);
    ans.setValues(val);
    return ans;
}

SparseMatrix SparseMatrix::threadMultiplication(const SparseMatrix & a) {
    if (size != a.getSize()) {
        throw(std::string)"Wrong matrix sizes";
    }
    SparseMatrix a_t  = a.transposition();
    auto a_t_values   = a_t.getValues();
    auto a_t_cols     = a_t.getCols();
    auto a_t_pointers = a_t.getPointers();
    size_t count      =
        static_cast<size_t>(std::thread::hardware_concurrency());
    int parts         = (size + count - 1) / count;
    std::vector<std::thread> tasks;
    std::vector<std::vector<int>> parallel_columns(size);
    std::vector<std::vector<std::complex<double>>> parallel_values(size);
    values.emplace_back(0, 0);
    int tmp = static_cast<int>(values.size() - 1);
    int start = 0;

    auto calc_thread = [&](int row1,
                           int end) {
        std::vector<int> used(size, tmp);
        std::complex<double> cur(0, 0);
        for (; row1 < end; ++row1) {
            for (int row2 = 0; row2 < size; ++row2) {
                cur = {0, 0};
                used.assign(size, tmp);
                int start = pointers[row1];
                int finish = pointers[row1 + 1];
                for (int i = start; i < finish; ++i) {
                    used[cols[i]] = i;
                }
                start = a_t_pointers[row2];
                finish = a_t_pointers[row2 + 1];
                for (int i = start; i < finish; ++i) {
                    cur += values[used[a_t_cols[i]]] * a_t_values[i];
                }
                if (!equalZero(cur)) {
                    parallel_columns[row1].emplace_back(row2);
                    parallel_values[row1].emplace_back(cur);
                }
            }
        }
    };

    for (size_t task = 0; task < count; ++task) {
        tasks.emplace_back(
            std::thread(calc_thread, start, (size < start + parts) ? size : start + parts));
        start += parts;
    }
    for (auto &thread : tasks) {
        thread.join();
    }
    values.pop_back();
    int sum = 0;
    for (int i = 0; i < size; ++i) {
        sum += static_cast<int>(parallel_columns[i].size());
    }
    SparseMatrix ans(size);
    ans.cols.reserve(sum);
    ans.values.reserve(sum);
    ans.pointers.reserve(size + 1);
    ans.pointers.emplace_back(0);
    for (int i = 0; i < size; ++i) {
        std::copy(parallel_columns[i].begin(),
                  parallel_columns[i].end(),
                  std::back_inserter(ans.cols));
        std::copy(parallel_values[i].begin(),
                  parallel_values[i].end(),
                  std::back_inserter(ans.values));
        ans.pointers.emplace_back(static_cast<int>(ans.cols.size()));
    }
    return ans;
}

std::vector<std::complex<double>>
    stupidDenseMultiplication(const std::vector<std::complex<double>> & a,
                              const std::vector<std::complex<double>> & b,
                              const int size) {
    if (a.size() != size * size || b.size() != size * size) {
        throw(std::string)"Wrong size of matrices";
    }
    std::vector<std::complex<double>> ans(size * size);
    for (int i = 0; i < size; ++i) {
        for (int j = 0; j < size; ++j) {
            for (int k = 0; k < size; ++k) {
                ans[i * size + j] += a[i * size + k] * b[k * size + j];
            }
        }
    }
    return ans;
}
\end{lstlisting}

\end{document}
